{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wfdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import spikingjelly.clock_driven as sj\n",
    "from spikingjelly.clock_driven import functional as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBXL(Dataset):\n",
    "    def __init__(self, csv_path, records_path, sampling_rate=100, split='train'):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.records_path = records_path\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "        # Filter only diagnostic labels\n",
    "        self.data.scp_codes = self.data.scp_codes.apply(eval) \n",
    "        agg_df = pd.read_csv(records_path.replace('records100', 'scp_statements.csv'), index_col=0)\n",
    "        agg_df = agg_df[agg_df.diagnostic == 1]\n",
    "        \n",
    "        def aggregate(y_dic):\n",
    "            return list(set([agg_df.loc[k].diagnostic_class for k in y_dic if k in agg_df.index]))\n",
    "        \n",
    "        self.data['diagnostic_superclass'] = self.data.scp_codes.apply(aggregate)\n",
    "\n",
    "        # Keep only samples with diagnostic labels\n",
    "        self.data = self.data[self.data.diagnostic_superclass.map(lambda d: len(d)) > 0]\n",
    "\n",
    "        # Filter out classes with only one sample\n",
    "        class_counts = self.data['diagnostic_superclass'].value_counts()\n",
    "        self.data = self.data[self.data['diagnostic_superclass'].isin(class_counts[class_counts > 1].index)]\n",
    "\n",
    "        # Now perform the split\n",
    "        train, test = train_test_split(self.data, test_size=0.2, random_state=42, stratify=self.data['diagnostic_superclass'])\n",
    "        self.data = train if split == 'train' else test\n",
    "\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.mlb.fit(self.data.diagnostic_superclass)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        record_path = f\"{self.records_path}/{row.filename_lr}\"\n",
    "        if 'records100/records100' in record_path:\n",
    "            record_path = record_path.replace('records100/records100', 'records100')\n",
    "        if '13000' in record_path:\n",
    "            return None\n",
    "\n",
    "        record_path = record_path.replace('hr', 'lr').replace('.hea', '')\n",
    "        try:\n",
    "            signal, _ = wfdb.rdsamp(record_path)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        # signal: shape (n_samples, n_channels) → transpose to (channels, length)\n",
    "        signal = signal.T  # (12, length)\n",
    "\n",
    "        # normalize\n",
    "        signal = (signal - signal.mean()) / signal.std()\n",
    "\n",
    "        # reshape to (channels, height, width) for Conv2d\n",
    "        signal = np.expand_dims(signal, axis=1)  # (12, 1, length)\n",
    "\n",
    "        label = self.mlb.transform([row.diagnostic_superclass])[0]\n",
    "\n",
    "        return torch.tensor(signal, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Filter out None values from the batch\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "\n",
    "    # Handle case where no valid samples are present in the batch\n",
    "    if len(batch) == 0:\n",
    "        return None, None  # Return None if there are no valid samples in the batch\n",
    "\n",
    "    # Stack the data and labels\n",
    "    data, targets = zip(*batch)\n",
    "    \n",
    "    # Ensure the batch sizes match\n",
    "    if len(data) != len(targets):\n",
    "        raise ValueError(f\"Input batch size ({len(data)}) doesn't match target batch size ({len(targets)})\")\n",
    "\n",
    "    data = torch.stack(data)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return data, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikingjelly.clock_driven.layer as sj_layer\n",
    "import torch.nn.functional as F\n",
    "class SpikingCAM(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SpikingCAM, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attention(x)\n",
    "\n",
    "class SpikingTwoStageSNN(nn.Module):\n",
    "    def __init__(self, input_channels=12, num_classes1=2, num_classes2=4):\n",
    "        super().__init__()\n",
    "        # Stage 1: normal vs abnormal\n",
    "        self.conv1_stage1 = sj_layer.ConvBatchNorm2d(input_channels, 32, kernel_size=5, padding=2)\n",
    "        self.cam1         = SpikingCAM(32)\n",
    "        self.conv2_stage1 = sj_layer.ConvBatchNorm2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.fc1          = nn.Linear(64, num_classes1)\n",
    "\n",
    "        # Stage 2: detailed abnormal classification\n",
    "        self.conv1_stage2 = sj_layer.ConvBatchNorm2d(input_channels, 32, kernel_size=5, padding=2)\n",
    "        self.cam2         = SpikingCAM(32)\n",
    "        self.conv2_stage2 = sj_layer.ConvBatchNorm2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.fc2          = nn.Linear(64, num_classes2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 12, 1, length)\n",
    "\n",
    "        # --- Stage 1 ---\n",
    "        x1 = F.relu(self.conv1_stage1(x))   # → (batch,32,1,length)\n",
    "        x1 = self.cam1(x1)                  # → same\n",
    "        x1 = F.relu(self.conv2_stage1(x1))  # → (batch,64,1,length)\n",
    "        x1 = x1.mean(dim=[2,3])             # global avg → (batch,64)\n",
    "        out1 = self.fc1(x1)                 # → (batch,2)\n",
    "\n",
    "        # --- Stage 2 ---\n",
    "        x2 = F.relu(self.conv1_stage2(x))   # → (batch,32,1,length)\n",
    "        x2 = self.cam2(x2)\n",
    "        x2 = F.relu(self.conv2_stage2(x2))  # → (batch,64,1,length)\n",
    "        x2 = x2.mean(dim=[2,3])             # → (batch,64)\n",
    "        out2 = self.fc2(x2)                 # → (batch,4)\n",
    "\n",
    "        return out1, out2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example function for accuracy calculation\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    _, predicted = torch.max(predictions, 1)\n",
    "    correct = (predicted == targets).sum().item()\n",
    "    accuracy = correct / targets.size(0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 12, 1, 1000]) torch.Size([15, 5])\n"
     ]
    }
   ],
   "source": [
    "# custom collate_fn to drop None\n",
    "def collate_fn(batch):\n",
    "    batch = [x for x in batch if x is not None]\n",
    "    data, labels = zip(*batch)\n",
    "    return torch.stack(data), torch.stack(labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "for data, labels in train_loader:\n",
    "    print(data.shape, labels.shape)\n",
    "    # expect: (16,12,1,1000), (16,<#classes>)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, patience, device):\n",
    "    best_accuracy = 0.0\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_stage1 = 0\n",
    "        total = 0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            # drop empty / bad batches\n",
    "            if data is None or targets is None:\n",
    "                continue\n",
    "\n",
    "            # move to device\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # --- forward ---\n",
    "            out1, out2 = model(data)\n",
    "            # out1: (batch, 2)  normal vs abnormal\n",
    "            # out2: (batch, 4)  detailed abnormal\n",
    "\n",
    "            # --- compute stage1 targets + loss ---\n",
    "            # Binary target for normal vs abnormal: if sum of multi-hot labels > 0, it's abnormal\n",
    "            y1 = (targets.sum(dim=1) > 0).long()  # 0 = normal, 1 = abnormal\n",
    "            loss1 = criterion(out1, y1)\n",
    "\n",
    "            # track stage1 accuracy\n",
    "            preds1 = out1.argmax(dim=1)\n",
    "            correct_stage1 += (preds1 == y1).sum().item()\n",
    "            total += y1.size(0)\n",
    "\n",
    "            # --- compute stage2 targets + loss (only on abnormal samples) ---\n",
    "            # Stage 2: Detailed classification for abnormal cases (only those that are abnormal)\n",
    "            idx_abn = (y1 == 1).nonzero(as_tuple=True)[0]  # indices of abnormal samples\n",
    "            if idx_abn.numel() > 0:\n",
    "                # select only abnormal samples\n",
    "                targ2_multi = targets[idx_abn]  # shape (n_abn, num_classes)\n",
    "                # map multi-hot to a single class 0–3: argmax over the abnormal subset\n",
    "                # (assuming positions 1–4 in `targets` correspond to the 4 abnormal classes)\n",
    "                y2 = targ2_multi[:, 1:].argmax(dim=1).long()  # Select class 1–4 for abnormal\n",
    "                loss2 = criterion(out2[idx_abn], y2)\n",
    "            else:\n",
    "                loss2 = torch.tensor(0.0, device=device)\n",
    "\n",
    "            # --- total loss & backward ---\n",
    "            loss = loss1 + loss2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # end epoch training\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        stage1_acc = 100.0 * correct_stage1 / total if total > 0 else 0.0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} — Loss: {avg_loss:.4f}, Stage1 Acc: {stage1_acc:.2f}%\")\n",
    "\n",
    "        # --- validation & early stopping based on stage1 acc ---\n",
    "        val_acc = evaluate_stage1(model, val_loader, device)  # Evaluate stage 1\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_stage1(model, loader, device):\n",
    "    \"\"\"\n",
    "    Compute the binary (normal vs abnormal) accuracy of stage‐1 head on the validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total   = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in loader:\n",
    "            # skip any bad batches\n",
    "            if data is None or targets is None:\n",
    "                continue\n",
    "\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            out1, _ = model(data)                 # ignore stage‐2\n",
    "            # build binary labels: sum>0 ⇒ abnormal(1), else normal(0)\n",
    "            y1 = (targets.sum(dim=1) > 0).long()   # shape (batch,)\n",
    "            preds1 = out1.argmax(dim=1)            # shape (batch,)\n",
    "            \n",
    "            correct += (preds1 == y1).sum().item()\n",
    "            total   += y1.size(0)\n",
    "\n",
    "    return 100.0 * correct / total if total>0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset and dataloaders\n",
    "train_dataset = PTBXL(csv_path=r'D:\\23020407 Dang Minh Nguyet\\Seminar\\BTL-Seminar\\data\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1\\ptbxl_database.csv', \n",
    "                      records_path=r'D:\\23020407 Dang Minh Nguyet\\Seminar\\BTL-Seminar\\data\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1\\records100', \n",
    "                      split='train')\n",
    "\n",
    "val_dataset = PTBXL(csv_path=r'D:\\23020407 Dang Minh Nguyet\\Seminar\\BTL-Seminar\\data\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1\\ptbxl_database.csv', \n",
    "                    records_path=r'D:\\23020407 Dang Minh Nguyet\\Seminar\\BTL-Seminar\\data\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1\\records100', \n",
    "                    split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, labels in train_loader:\n",
    "    print(\"raw multi-hot labels:\", labels[:10])\n",
    "    y1 = (labels.sum(dim=1) > 0).long()\n",
    "    print(\"binary y1 labels:\", y1[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 — Loss: 0.9598, Stage1 Acc: 99.81%\n"
     ]
    }
   ],
   "source": [
    "# Define the model, optimizer, and loss function\n",
    "device = torch.device('cpu')  # Use 'cuda' if available\n",
    "model = SpikingTwoStageSNN()\n",
    "model.to(device)\n",
    "\n",
    "# Use CrossEntropyLoss for multi-class classification if applicable\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()  # Use this for multi-class classification\n",
    "\n",
    "# Training loop with early stopping and accuracy checking\n",
    "trained_model = train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=50, patience=5, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
